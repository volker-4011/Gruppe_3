---
title: "Support Vector Machine"
output: html_notebook
---


## Imports

```{r}
# Importing Function Packages
source("prep_environment.R")
source("prep_data.R")
```


```{r}
#Datei öffnen und einzeln laden source("feiertage.R")
```
```{r}
#Datei öffnen und einzeln laden source("ferientage.R")
```


```{r Beispiel}
house_pricing <- read_csv("https://raw.githubusercontent.com/opencampus-sh/einfuehrung-in-data-science-und-ml/main/Woche%206/house_pricing.csv")
```

```{r Beispiel}
fullData$Umsatz <- round(fullData$Umsatz)
fullData[is.na(fullData)] <- 0
```

## Splitting Training and Test Data

```{r}
# Setting the random counter to a fixed value, so the random initialization stays the same (the random split is always the same)
set.seed(1)
# Shuffling the dataset (to get random orders within each dataset as well)
new_row_order <- sample(nrow(fullData))
fullData <- fullData[new_row_order,]
# Assign each row number in the full dataset randomly to one of the three groups of datasets
# The probability of being in one of the groups results then in crresponding group sizes
assignment <- sample(1:2, size = nrow(fullData), prob = c(.9, .1), replace = TRUE)
# Create training and test datasets
train_dataset <- fullData[assignment == 1, ]  # subset house_pricing to training indices only
test_dataset <- fullData[assignment == 2, ]  # subset house_pricing to test indices only
```


## Data Preparation

```{r}
# Uncomment the next line if you want to check the correctness of your following code for the svm estimation with a small (and computationally fast) part of the training data set
train_dataset <- sample_frac(train_dataset, .10)
```


## Training the SVM

```{r}
# Optimization of an SVM with standard hyper parameters
# Typically NOT used; Instead, the function svm_tune() is used in order to also get a model with optimized hyper parameters
model_svm <- svm(Umsatz ~ Warengruppe, train_dataset)
```





```{r}
# Optimization of various SVM using systematically varied hyper parameters (typically called 'grid search' approach) and cross validation
# the resulting object includes the optimal model in the element named 'best.model'
svm_tune <- tune(svm, Umsatz ~ Bewoelkung + Warengruppe, data=train_dataset,
                 ranges = list(epsilon = seq(0.2,1,0.1), cost = 2^(2:3)))
```


## Checking the Prediction Quality


### Trainig Data

SVM with a standard hyperparameters
```{r}
# Calculating the prediction for the training data using the best model according to the grid search
pred_train <- predict(model_svm, train_dataset)
# Calculating the prediction quality for the training data using the MAPE
mape(train_dataset$Umsatz, pred_train)
```

SVM with hyperparameters tuned via grid search and cross validation
```{r}
# Calculating the prediction for the training data using the best model according to the grid search
pred_train <- predict(svm_tune$best.model, train_dataset)
# Calculating the prediction quality for the training data using the MAPE
mape(train_dataset$Umsatz, pred_train)
```

### Test Data

SVM with a standard hyperparameters
```{r}
# Calculating the prediction for the validation data using the best model according to the grid search
pred_test <- predict(model_svm, test_dataset)
# Calculating the prediction quality for the validation data using the MAPE
mape(test_dataset$Umsatz, pred_test)
```

SVM with hyperparameters tuned via grid search and cross validation
```{r}
# Calculating the prediction for the test data using the best model according to the grid search
pred_test <- predict(svm_tune$best.model, test_dataset)
# Calculating the prediction quality for the test data using the MAPE
mape(test_dataset$Umsatz, pred_test)
```


```{r}
ddply(fullData, .(Datum), nrow)
```


```{r}
nd <- 1:2122
```

```{r}
#plot the results
ylim <- c(min(fullData$Datum), max(fullData$Datum))
xlim <- c(min(nd),max(nd))
plot(fullData$Datum, col="blue", ylim=ylim, xlim=xlim, type="l")
par(new=TRUE)
plot(pred_test, col="red", ylim=ylim, xlim=xlim)
```

